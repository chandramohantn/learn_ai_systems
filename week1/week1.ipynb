{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ml6UYUncz3Tk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G4-CoMp0I1E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Converts input token indices into dense vector representations\n",
        "\n",
        "    Why its important: Models process number and not text. Embeddings map tokens to a continuous vector space where semantic similarity is reflected by distance and direction.\n",
        "    How we build it: We use PyTorch's nn.Embedding layer and scale the outputs by the square root of the embedding dimension. This is mainly for training stability.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.embedding_size)"
      ],
      "metadata": {
        "id": "qMuFv3xF0I9I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes the relative or absolute position of tokens in the sequence using sinusoidal functions\n",
        "\n",
        "    Why its important: The attention mechanism is permutation invariant. Without positional encoding, the model will treat the sequence as a bag of words.\n",
        "    How we build it: We use sine and cosine functions of different frequencies to generate a unique encoding for each position. These are added to token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, dropout: float, sequence_len: int):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a positional encoding matrix of shape (sequence_len, embedding_size)\n",
        "        positional_encoding = torch.zeros(sequence_len, embedding_size)\n",
        "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (sequence_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        positional_encoding = positional_encoding.unsqueeze(0) # (1, sequence_len, embedding_size) for batch broadcasting\n",
        "\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding) # Not a model parameter but a part of the state\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_len, embedding_size)\n",
        "        \"\"\"\n",
        "        x = x + self.positional_encoding[:, :x.size(1)] # Add positional encoding\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "HGNp6CdI0I_t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalizes the inputs across the feature dimension for each data point in the batch independently\n",
        "\n",
        "    Why its important: Reduces internal covariate shift and thus helps in stabilizing and accelerating training\n",
        "    How we build it: Compute mean and std for each input across its feature dimension\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(embedding_size)) # Learnable scale\n",
        "        self.bias = nn.Parameter(torch.zeros(embedding_size)) # Learnable shift\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdims=True)\n",
        "        std = x.std(dim=-1, keepdims=True, unbiased=False)\n",
        "        normalised = (x - mean) / (std + self.eps)\n",
        "        return self.alpha * normalised + self.bias"
      ],
      "metadata": {
        "id": "Z8AZgWYkVUXr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously.\n",
        "    ANALOGY: Researching a topic (query) when you have multiple books (keys) with different content (values). Attention is like deciding which books are relevant and how much to read from each.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert embedding_size % n_heads == 0, \"Embedding size must be divisible by n_heads\"\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_heads = n_heads\n",
        "        self.dimensions_per_head = embedding_size // n_heads # Dimensions per head\n",
        "\n",
        "        # Why separate projects? Each head learns different aspects\n",
        "        self.w_q = nn.Linear(embedding_size, embedding_size) # Query projection\n",
        "        self.w_k = nn.Linear(embedding_size, embedding_size) # Key projection\n",
        "        self.w_v = nn.Linear(embedding_size, embedding_size) # Value projection\n",
        "        self.w_o = nn.Linear(embedding_size, embedding_size) # Output projection\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Attention mechanism: Core calculation\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask=None, dropout=None):\n",
        "        \"\"\"\n",
        "        Computes the scaled dot product attention\n",
        "        \"\"\"\n",
        "        head_dimension = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(head_dimension)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_weights = dropout(attention_weights)\n",
        "\n",
        "        return torch.matmul(attention_weights, value), attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Query, Key, Value: Tensors of shape (batch_size, seq_len, embedding_size)\n",
        "        mask: To prevent attention of certain positions\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections and split into heads\n",
        "        query = self.w_q(query).view(batch_size, -1, self.n_heads, self.dimensions_per_head).transpose(1, 2)\n",
        "        key = self.w_k(key).view(batch_size, -1, self.n_heads, self.dimensions_per_head).transpose(1, 2)\n",
        "        value = self.w_v(value).view(batch_size, -1, self.n_heads, self.dimensions_per_head).transpose(1, 2)\n",
        "\n",
        "        # Apply attention\n",
        "        x, self.attention_weights = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Concatenate heads and put through final linear layer\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_size)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "jGxKWcTYVUZr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feed-forward network applied to each position in the sequence independently\n",
        "\n",
        "    Why its important: The self attention output is a linear combination of values. The FFN introduces non-linearity and allows for more complex transformations.\n",
        "    How we build it: We use two linear layers with an expansion factor (4x) and a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embedding_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(self.activation(self.linear1(x))))"
      ],
      "metadata": {
        "id": "ICsb3XQ_0JCT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines multi-head attention and the feed-forward network with residual connections and layer normalization.\n",
        "\n",
        "    Why its important: Transforms input sequences into contextualized representations. Stacking blocks allows the model to build up increasingly abstract and complex representations of the input.\n",
        "    How we build it: Wrap the MultiHeadAttention and PositionWiseFFN with residual connections and layer norm for stable training.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, n_head: int, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(embedding_size, n_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFFN(embedding_size, hidden_size, dropout)\n",
        "        self.norm1 = LayerNormalization(embedding_size)\n",
        "        self.norm2 = LayerNormalization(embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self attention with residual connection and layer norm\n",
        "        attended = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(feed_forward))\n",
        "        return x"
      ],
      "metadata": {
        "id": "8Mb-GpU8-Sav"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Similar to encoder but includes an additional cross-attention layer to attend to the encoders output.\n",
        "    A single block of decoder comprising of self-attention, cross-attention (encoder-decoder attention) and a feed-forward layer.\n",
        "\n",
        "    Why its important: It generates output sequence one token at a time, using both the previously generated tokens (self-attention) and the encoded input (cross-attention).\n",
        "        For sequence-to-sequence tasks like translation, the decoder must condition its output on both target sequence so far and the entire input sequence.\n",
        "    How we build it: Add a third sublayer for encoder-decoder attention on top of the two sublayers found in the encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, n_heads: int, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(embedding_size, n_heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttention(embedding_size, n_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFFN(embedding_size, n_heads, dropout)\n",
        "        self.norm1 = LayerNormalization(embedding_size)\n",
        "        self.norm2 = LayerNormalization(embedding_size)\n",
        "        self.norm3 = LayerNormalization(embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # Masked sef-attention (prevents attending to future tokens)\n",
        "        attended_self = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attended_self))\n",
        "\n",
        "        # Cross-attention to encoder output\n",
        "        attended_cross = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attended_cross))\n",
        "\n",
        "        # Feed-forward\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(feed_forward))\n",
        "        return x"
      ],
      "metadata": {
        "id": "FO2c41O-mvzy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine embedding layers, stack of encoder blocks and a stack of decoder blocks\n",
        "\n",
        "    Why its important: Integrate all components for end to end training.\n",
        "    How we build it: Chain embedding layers, encoder (stack of N encoder blocks), decoder (stack of N decoder blocks) followed by final linear projection.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, src_max_len: int, tgt_max_len: int, embedding_size: int, n_heads: int, layers: int, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = InputEmbeddings(embedding_size, src_vocab_size)\n",
        "        self.decoder_embedding = InputEmbeddings(embedding_size, tgt_vocab_size)\n",
        "        self.src_pos_encoding = PositionalEncoding(embedding_size, dropout, src_max_len)\n",
        "        self.tgt_pos_encoding = PositionalEncoding(embedding_size, dropout, tgt_max_len)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([EncoderBlock(embedding_size, n_heads, hidden_size, dropout) for _ in range(layers)])\n",
        "        self.decoder_blocks = nn.ModuleList([DecoderBlock(embedding_size, n_heads, hidden_size, dropout) for _ in range(layers)])\n",
        "\n",
        "        self.final_linear = nn.Linear(embedding_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src_embedded = self.dropout(self.src_pos_encoding(self.encoder_embedding(src)))\n",
        "        for block in self.encoder_blocks:\n",
        "            src_embedded = block(src_embedded, src_mask)\n",
        "        return src_embedded\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "        tgt_embedded = self.dropout(self.tgt_pos_encoding(self.decoder_embedding(tgt)))\n",
        "        for block in self.decoder_blocks:\n",
        "            tgt_embedded = block(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
        "        return tgt_embedded\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "        return self.final_linear(decoder_output)"
      ],
      "metadata": {
        "id": "FyWEk-5vmv2D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dataset_name = 'Trelis/tiny-shakespeare'\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# Access the splits\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# To see an example from the training set\n",
        "print(train_dataset[0]['Text'])\n",
        "\n",
        "# Combine all text from training dataset to build vocabulary\n",
        "text_data = \"\".join([example['Text'] for example in train_dataset])\n",
        "\n",
        "# Create a character level vocabulary\n",
        "vocab = sorted(list(set(text_data)))\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {chr: idx for idx, chr in enumerate(vocab)}\n",
        "idx_to_char = {idx: chr for idx, chr in enumerate(vocab)}\n",
        "\n",
        "# Encode the entire dataset\n",
        "def encode_text(text):\n",
        "    return torch.tensor([char_to_idx.get(ch, 0) for ch in text], dtype=torch.long)\n",
        "\n",
        "# Encode train and test data\n",
        "train_text = \"\".join([example['Text'] for example in train_dataset])\n",
        "test_text = \"\".join([example['Text'] for example in test_dataset])\n",
        "\n",
        "train_data = encode_text(train_text)\n",
        "val_data = encode_text(test_text)\n",
        "\n",
        "print(f\"Training data length: {len(train_data)}\")\n",
        "print(f\"Validation data length: {len(val_data)}\")\n",
        "\n",
        "def get_batch(split, batch_size, block_size):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    # Ensure we don't go out of bounds\n",
        "    max_start_idx = len(data) - block_size - 1\n",
        "    if max_start_idx <= 0:\n",
        "        raise ValueError(f\"Data too short for block_size {block_size}\")\n",
        "\n",
        "    idx = torch.randint(0, max_start_idx, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cDp6IjxM-SdR",
        "outputId": "df32bd5d-6c2d-4987-dcdd-6d6566674f29"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall know we\n",
            "have strong arms too.\n",
            "\n",
            "MENENIUS:\n",
            "Why, masters, my good friends, mine honest neighbours,\n",
            "Will you undo yourselves?\n",
            "\n",
            "First Citizen:\n",
            "We cannot, sir, we are undone already.\n",
            "\n",
            "MENENIUS:\n",
            "I tell you, friends, most charitable care\n",
            "Have the patricians of you. For your wants,\n",
            "Your suffering in this dearth, you may as well\n",
            "Strike at the heaven with your staves as lift them\n",
            "Against the Roman state, whose course will on\n",
            "The way it takes, cracking ten thousand curbs\n",
            "Of more strong link asunder than can ever\n",
            "Appear in your impediment. For the dearth,\n",
            "The gods, not the patricians, make it, and\n",
            "Your knees to them, not arms, must help.\n",
            "Training data length: 1222354\n",
            "Validation data length: 119020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "embedding_size = 256\n",
        "n_heads = 8\n",
        "layers = 6\n",
        "hidden_size = 1024\n",
        "dropout = 0.1\n",
        "learning_rate = 0.0003\n",
        "epochs = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Initialize model, loss and optimizer\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    src_max_len=block_size,\n",
        "    tgt_max_len=block_size,\n",
        "    embedding_size=embedding_size,\n",
        "    n_heads=n_heads,\n",
        "    layers=layers,\n",
        "    hidden_size=hidden_size,\n",
        "    dropout=dropout\n",
        ").to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYa0okFC8v8Z",
        "outputId": "c0666e51-f07a-4c13-b666-900e76c2b0e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model parameters: 7,981,937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    x, y = get_batch('train', batch_size, block_size)\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    # For language modeling, we use the same sequence for src and tgt\n",
        "    # But we need to shift the target for teacher forcing\n",
        "    logits = model(x, x[:, :-1])  # src: full sequence, tgt: sequence without last token\n",
        "\n",
        "    # Reshape for loss calculation\n",
        "    # logits: (batch_size, seq_len-1, vocab_size)\n",
        "    # targets: (batch_size, seq_len-1)\n",
        "    loss = criterion(logits.reshape(-1, vocab_size), y[:, 1:].reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lYiU_F0cp2H",
        "outputId": "b822c73a-adea-4349-c6b8-16a44d9a1c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 4.3385\n",
            "Epoch 2/100, Loss: 3.6365\n",
            "Epoch 3/100, Loss: 3.5255\n",
            "Epoch 4/100, Loss: 3.4619\n",
            "Epoch 5/100, Loss: 3.3663\n",
            "Epoch 6/100, Loss: 3.3703\n",
            "Epoch 7/100, Loss: 3.3303\n",
            "Epoch 8/100, Loss: 3.3616\n",
            "Epoch 9/100, Loss: 3.3421\n",
            "Epoch 10/100, Loss: 3.3060\n",
            "Epoch 11/100, Loss: 3.3744\n",
            "Epoch 12/100, Loss: 3.3474\n",
            "Epoch 13/100, Loss: 3.3089\n",
            "Epoch 14/100, Loss: 3.3364\n",
            "Epoch 15/100, Loss: 3.3359\n",
            "Epoch 16/100, Loss: 3.3264\n",
            "Epoch 17/100, Loss: 3.3231\n",
            "Epoch 18/100, Loss: 3.3204\n",
            "Epoch 19/100, Loss: 3.2921\n",
            "Epoch 20/100, Loss: 3.2990\n",
            "Epoch 21/100, Loss: 3.2822\n",
            "Epoch 22/100, Loss: 3.3046\n",
            "Epoch 23/100, Loss: 3.2927\n",
            "Epoch 24/100, Loss: 3.2764\n",
            "Epoch 25/100, Loss: 3.3526\n",
            "Epoch 26/100, Loss: 3.3020\n",
            "Epoch 27/100, Loss: 3.2281\n",
            "Epoch 28/100, Loss: 3.2731\n",
            "Epoch 29/100, Loss: 3.2444\n",
            "Epoch 30/100, Loss: 3.2395\n",
            "Epoch 31/100, Loss: 3.2090\n",
            "Epoch 32/100, Loss: 3.2058\n",
            "Epoch 33/100, Loss: 3.2784\n",
            "Epoch 34/100, Loss: 3.2741\n",
            "Epoch 35/100, Loss: 3.1877\n",
            "Epoch 36/100, Loss: 3.2211\n",
            "Epoch 37/100, Loss: 3.2206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def generate_text(model, start_string, max_length=100):\n",
        "    model.eval()\n",
        "    input_ids = encode_text(start_string).unsqueeze(0).to(DEVICE)\n",
        "    generated = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            logits = model(generated, generated[:, :-1])\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "    return \"\".join([idx_to_char[idx.item()] for idx in generated[0]])\n",
        "\n",
        "# Generate some text after training\n",
        "test_start = \"KING:\"\n",
        "generated_text = generate_text(model, test_start, max_length=50)\n",
        "print(f\"\\nGenerated text starting with '{test_start}':\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "S3a-LRBjf5n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_Atap91mTjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}