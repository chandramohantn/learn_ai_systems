{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ml6UYUncz3Tk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G4-CoMp0I1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Converts input token indices into dense vector representations\n",
        "\n",
        "    Why its important: Models process number and not text. Embeddings map tokens to a continuous vector space where semantic similarity is reflected by distance and direction.\n",
        "    How we build it: We use PyTorch's nn.Embedding layer and scale the outputs by the square root of the embedding dimension. This is mainly for training stability.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.embedding_size)"
      ],
      "metadata": {
        "id": "qMuFv3xF0I9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes the relative or absolute position of tokens in the sequence using sinusoidal functions\n",
        "\n",
        "    Why its important: The attention mechanism is permutation invariant. Without positional encoding, the model will treat the sequence as a bag of words.\n",
        "    How we build it: We use sine and cosine functions of different frequencies to generate a unique encoding for each position. These are added to token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, dropout: float, sequence_len: int):\n",
        "        super.__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a positional encoding matrix of shape (sequence_len, embedding_size)\n",
        "        positional_encoding = torch.zeros(sequence_len, embedding_size)\n",
        "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (sequence_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        positional_encoding = positional_encoding.unsqueeze(0) # (1, sequence_len, embedding_size) for batch broadcasting\n",
        "\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding) # Not a model parameter but a part of the state\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_len, embedding_size)\n",
        "        \"\"\"\n",
        "        x = x + self.positional_encoding[:, :x.size(1)] # Add positional encoding\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "HGNp6CdI0I_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalizes the inputs across the feature dimension for each data point in the batch independently\n",
        "\n",
        "    Why its important: Reduces internal covariate shift and thus helps in stabilizing and accelerating training\n",
        "    How we build it: Compute mean and std for each input across its feature dimension\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, eps: float = 1e-6):\n",
        "        super.__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(embedding_size)) # Learnable scale\n",
        "        self.bias = nn.Parameter(torch.zeros(embedding_size)) # Learnable shift\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdims=True)\n",
        "        std = x.std(dim=-1, keepdims=True, unbiased=False)\n",
        "        normalised = (x - mean) / (std + self.eps)\n",
        "        return self.alpha * normalised + self.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8AZgWYkVUXr",
        "outputId": "57a264cc-5285-4ae3-8dc3-f866d0ea50a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously.\n",
        "    ANALOGY: Researching a topic (query) when you have multiple books (keys) with different content (values). Attention is like deciding which books are relevant and how much to read from each.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size: int, n_heads: int, dropout: float):\n",
        "        super.__init__()\n",
        "        assert embedding_size % n_heads == 0, \"Embedding size must be divisible by n_heads\"\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_heads = n_heads\n",
        "        self.dimensions_per_head = embedding_size // n_heads # Dimensions per head\n",
        "\n",
        "        # Why separate projects? Each head learns different aspects\n",
        "        self.w_q = nn.Linear(embedding_size, embedding_size) # Query projection\n",
        "        self.w_k = nn.Linear(embedding_size, embedding_size) # Key projection\n",
        "        self.w_v = nn.Linear(embedding_size, embedding_size) # Value projection\n",
        "        self.w_o = nn.Linear(embedding_size, embedding_size) # Output projection\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Attention mechanism: Core calculation"
      ],
      "metadata": {
        "id": "jGxKWcTYVUZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICsb3XQ_0JCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}